"""STORM integration for generating comprehensive P2025 policy reports.

Uses Stanford OVAL's STORM (Synthesis of Topic Outlines through Retrieval
and Multi-perspective Question Asking) to generate Wikipedia-style articles
about P2025 policies with citations.

STORM can be grounded on:
1. Internet search (default) - Uses web search APIs
2. Custom documents (VectorRM) - Uses our P2025 policy database

Requirements:
    pip install knowledge-storm qdrant-client

Environment variables:
    OPENAI_API_KEY - Required for LLM (or use Ollama)
    BING_SEARCH_API_KEY - For web search (optional)

Usage:
    from civitas.research.storm_integration import STORMReportGenerator

    generator = STORMReportGenerator(session)
    report = generator.generate_policy_report(
        topic="EPA environmental regulations under Project 2025",
        use_custom_docs=True,
    )
    print(report.article)

References:
    - https://github.com/stanford-oval/storm
    - https://storm.genie.stanford.edu/
"""

import csv
import json
import os
import tempfile
from dataclasses import dataclass
from typing import Any

from sqlalchemy.orm import Session


@dataclass
class STORMReport:
    """A report generated by STORM."""

    topic: str
    outline: str | None = None
    article: str | None = None
    references: list[dict] | None = None
    conversation_log: str | None = None
    output_dir: str | None = None


class STORMReportGenerator:
    """Generates comprehensive reports using STORM.

    STORM conducts research by simulating conversations between
    Wikipedia writers and topic experts, then synthesizes findings
    into a well-structured article with citations.

    For P2025 analysis, it can be grounded on:
    - Our policy database (VectorRM mode)
    - Web search for current events and analysis
    - Hybrid approach combining both

    Example:
        generator = STORMReportGenerator(session)

        # Generate report on a specific policy area
        report = generator.generate_policy_report(
            topic="How does Project 2025 propose to restructure the EPA?",
            use_custom_docs=True,
        )

        # Generate report with web search for current context
        report = generator.generate_policy_report(
            topic="State responses to Project 2025 immigration policies",
            use_web_search=True,
        )
    """

    def __init__(
        self,
        db_session: Session,
        output_dir: str | None = None,
        openai_api_key: str | None = None,
        ollama_host: str | None = None,
    ):
        """Initialize the STORM report generator.

        Args:
            db_session: SQLAlchemy database session
            output_dir: Directory for STORM outputs (default: temp dir)
            openai_api_key: OpenAI API key (default: from env)
            ollama_host: Ollama host for local LLM (alternative to OpenAI)
        """
        self.session = db_session
        self.output_dir = output_dir or tempfile.mkdtemp(prefix="storm_")
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        self.ollama_host = ollama_host or os.getenv("OLLAMA_HOST")

        # Lazy import check
        self._storm_available = None
        self._vector_db_path = None

    def _check_storm_available(self) -> bool:
        """Check if STORM is installed."""
        if self._storm_available is not None:
            return self._storm_available

        try:
            import knowledge_storm  # noqa: F401

            self._storm_available = True
        except ImportError:
            self._storm_available = False

        return self._storm_available

    def _ensure_storm(self):
        """Ensure STORM is available, raise helpful error if not."""
        if not self._check_storm_available():
            raise ImportError(
                "STORM is not installed. Install with:\n"
                "  pip install knowledge-storm qdrant-client\n\n"
                "For more info: https://github.com/stanford-oval/storm"
            )

    def export_policies_for_storm(
        self,
        output_path: str | None = None,
        include_analyses: bool = True,
        include_recommendations: bool = True,
    ) -> str:
        """Export P2025 policies to CSV for STORM VectorRM.

        Creates a CSV file with the required fields for STORM:
        - content: Policy text and analysis
        - title: Policy title
        - url: Unique identifier
        - description: Policy summary

        Args:
            output_path: Path for CSV file (default: temp file)
            include_analyses: Include resistance analyses
            include_recommendations: Include recommendations

        Returns:
            Path to the generated CSV file
        """
        from civitas.db.models import (
            Project2025Policy,
            ResistanceAnalysis,
            ResistanceRecommendation,
        )

        if output_path is None:
            output_path = os.path.join(self.output_dir, "p2025_policies.csv")

        policies = self.session.query(Project2025Policy).all()

        rows = []
        for policy in policies:
            # Build comprehensive content
            content_parts = [
                f"Agency: {policy.agency}",
                f"Section: {policy.section}",
                f"Category: {policy.category}",
                "",
                "Policy Proposal:",
                policy.proposal_text or "",
            ]

            # Add analysis if available
            if include_analyses:
                analysis = (
                    self.session.query(ResistanceAnalysis)
                    .filter(ResistanceAnalysis.p2025_policy_id == policy.id)
                    .first()
                )
                if analysis and analysis.analysis_json:
                    try:
                        analysis_data = json.loads(analysis.analysis_json)
                        vuln_score = analysis_data.get(
                            'overall_vulnerability_score', 'N/A'
                        )
                        content_parts.extend([
                            "",
                            "Legal Analysis:",
                            f"Vulnerability Score: {vuln_score}/100",
                        ])

                        # Add constitutional issues
                        issues = analysis_data.get("constitutional_issues", [])
                        if issues:
                            content_parts.append("\nConstitutional Issues:")
                            for issue in issues[:3]:
                                prov = issue.get('provision', 'Unknown')
                                iss = issue.get('issue', '')
                                content_parts.append(f"- {prov}: {iss}")

                        # Add challenge strategies
                        strategies = analysis_data.get("challenge_strategies", [])
                        if strategies:
                            content_parts.append("\nChallenge Strategies:")
                            for strat in strategies[:3]:
                                s_type = strat.get('type', 'Unknown')
                                s_expl = strat.get('explanation', '')[:200]
                                content_parts.append(f"- {s_type}: {s_expl}")
                    except json.JSONDecodeError:
                        pass

            # Add recommendations if available
            if include_recommendations:
                recommendations = (
                    self.session.query(ResistanceRecommendation)
                    .filter(ResistanceRecommendation.p2025_policy_id == policy.id)
                    .limit(5)
                    .all()
                )
                if recommendations:
                    content_parts.extend(["", "Resistance Recommendations:"])
                    for rec in recommendations:
                        desc = rec.description[:200] if rec.description else ''
                        content_parts.append(f"- [{rec.tier}] {rec.title}: {desc}")

            content = "\n".join(content_parts)

            title = policy.short_title
            if not title:
                title = f"P2025 Policy: {policy.agency} - {policy.section[:50]}"
            desc = ""
            if policy.proposal_summary:
                desc = policy.proposal_summary
            elif policy.proposal_text:
                desc = policy.proposal_text[:200]

            rows.append({
                "url": f"civitas://p2025/policy/{policy.id}",
                "title": title,
                "description": desc,
                "content": content,
            })

        # Also add executive orders for context
        from civitas.db.models import ExecutiveOrder

        eos = self.session.query(ExecutiveOrder).limit(100).all()
        for eo in eos:
            eo_desc = eo.summary[:200] if eo.summary else ""
            eo_content = (
                f"Executive Order {eo.eo_number}\n\n"
                f"Title: {eo.title}\n\n"
                f"Summary: {eo.summary or 'No summary available'}"
            )
            rows.append({
                "url": f"civitas://eo/{eo.id}",
                "title": f"Executive Order: {eo.title or eo.eo_number}",
                "description": eo_desc,
                "content": eo_content,
            })

        # Write CSV
        with open(output_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=["url", "title", "description", "content"])
            writer.writeheader()
            writer.writerows(rows)

        return output_path

    def _setup_vector_db(self, csv_path: str, force_rebuild: bool = False) -> str:
        """Set up Qdrant vector database from CSV.

        Args:
            csv_path: Path to CSV file with documents
            force_rebuild: Force rebuild even if exists

        Returns:
            Path to vector database
        """
        vector_db_path = os.path.join(self.output_dir, "vector_db")

        if os.path.exists(vector_db_path) and not force_rebuild:
            return vector_db_path

        # Build vector database using STORM's utilities
        from knowledge_storm.rm import VectorRM

        # Initialize VectorRM which will create the database
        vector_rm = VectorRM(
            collection_name="civitas_p2025",
            embedding_model="BAAI/bge-m3",
            device="cpu",  # Use CPU for compatibility
        )

        # Initialize offline database
        vector_rm.init_offline_vector_db(vector_db_path)

        # Load documents from CSV
        import pandas as pd

        df = pd.read_csv(csv_path)

        # Add documents to the vector store
        from langchain.docstore.document import Document

        documents = [
            Document(
                page_content=row["content"],
                metadata={
                    "url": row["url"],
                    "title": row["title"],
                    "description": row.get("description", ""),
                },
            )
            for _, row in df.iterrows()
        ]

        # Add to Qdrant
        vector_rm.qdrant_client.add(
            collection_name="civitas_p2025",
            documents=[doc.page_content for doc in documents],
            metadata=[doc.metadata for doc in documents],
        )

        return vector_db_path

    def _create_lm_configs(self, use_ollama: bool = False) -> Any:
        """Create language model configurations for STORM.

        Args:
            use_ollama: Use Ollama instead of OpenAI

        Returns:
            STORMWikiLMConfigs object
        """
        from knowledge_storm import STORMWikiLMConfigs
        from knowledge_storm.lm import LitellmModel

        lm_configs = STORMWikiLMConfigs()

        if use_ollama and self.ollama_host:
            # Configure for Ollama
            # Note: Requires litellm ollama support
            model_name = os.getenv("OLLAMA_MODEL", "llama3.1:8b-instruct-q8_0")

            conv_lm = LitellmModel(
                model=f"ollama/{model_name}",
                api_base=self.ollama_host,
                max_tokens=500,
            )
            article_lm = LitellmModel(
                model=f"ollama/{model_name}",
                api_base=self.ollama_host,
                max_tokens=3000,
            )
        else:
            # Configure for OpenAI
            conv_lm = LitellmModel(model="gpt-3.5-turbo", max_tokens=500)
            article_lm = LitellmModel(model="gpt-4o", max_tokens=3000)

        # Set models for different stages
        lm_configs.set_conv_simulator_lm(conv_lm)
        lm_configs.set_question_asker_lm(conv_lm)
        lm_configs.set_outline_gen_lm(article_lm)
        lm_configs.set_article_gen_lm(article_lm)
        lm_configs.set_article_polish_lm(article_lm)

        return lm_configs

    def generate_policy_report(
        self,
        topic: str,
        use_custom_docs: bool = True,
        use_web_search: bool = False,
        use_ollama: bool = False,
        max_conv_turns: int = 3,
        max_perspectives: int = 3,
        search_top_k: int = 5,
    ) -> STORMReport:
        """Generate a comprehensive report on a P2025-related topic.

        Args:
            topic: The topic to research and write about
            use_custom_docs: Use P2025 policy database (VectorRM)
            use_web_search: Also use web search for current context
            use_ollama: Use Ollama instead of OpenAI
            max_conv_turns: Maximum conversation turns for research
            max_perspectives: Number of perspectives to explore
            search_top_k: Number of search results per query

        Returns:
            STORMReport with the generated article and metadata
        """
        self._ensure_storm()

        from knowledge_storm import STORMWikiRunner, STORMWikiRunnerArguments

        # Create output directory for this topic
        topic_slug = topic.lower().replace(" ", "_")[:50]
        topic_output_dir = os.path.join(self.output_dir, topic_slug)
        os.makedirs(topic_output_dir, exist_ok=True)

        # Set up language models
        lm_configs = self._create_lm_configs(use_ollama=use_ollama)

        # Set up retrieval module
        if use_custom_docs:
            # Export policies and set up vector DB
            csv_path = self.export_policies_for_storm()
            vector_db_path = self._setup_vector_db(csv_path)

            from knowledge_storm.rm import VectorRM

            rm = VectorRM(
                collection_name="civitas_p2025",
                embedding_model="BAAI/bge-m3",
                device="cpu",
                k=search_top_k,
            )
            rm.init_offline_vector_db(vector_db_path)
        elif use_web_search:
            # Use web search
            bing_api_key = os.getenv("BING_SEARCH_API_KEY")
            if bing_api_key:
                from knowledge_storm.rm import BingSearch

                rm = BingSearch(bing_search_api=bing_api_key, k=search_top_k)
            else:
                from knowledge_storm.rm import DuckDuckGoSearchRM

                rm = DuckDuckGoSearchRM(k=search_top_k)
        else:
            raise ValueError("Must enable either use_custom_docs or use_web_search")

        # Configure runner arguments
        runner_args = STORMWikiRunnerArguments(
            output_dir=topic_output_dir,
            max_conv_turn=max_conv_turns,
            max_perspective=max_perspectives,
            search_top_k=search_top_k,
        )

        # Create and run STORM
        runner = STORMWikiRunner(runner_args, lm_configs, rm)

        # Run the full pipeline
        runner.run(
            topic=topic,
            do_research=True,
            do_generate_outline=True,
            do_generate_article=True,
            do_polish_article=True,
        )

        # Collect results
        report = STORMReport(
            topic=topic,
            output_dir=topic_output_dir,
        )

        # Read generated files
        outline_path = os.path.join(topic_output_dir, "storm_gen_outline.txt")
        if os.path.exists(outline_path):
            with open(outline_path) as f:
                report.outline = f.read()

        article_path = os.path.join(topic_output_dir, "storm_gen_article_polished.txt")
        if not os.path.exists(article_path):
            article_path = os.path.join(topic_output_dir, "storm_gen_article.txt")
        if os.path.exists(article_path):
            with open(article_path) as f:
                report.article = f.read()

        # Read conversation log
        conv_path = os.path.join(topic_output_dir, "conversation_log.json")
        if os.path.exists(conv_path):
            with open(conv_path) as f:
                report.conversation_log = f.read()

        return report

    def generate_category_report(
        self,
        category: str,
        use_ollama: bool = False,
    ) -> STORMReport:
        """Generate a report for a P2025 policy category.

        Convenience method that formulates an appropriate topic
        for the given category.

        Args:
            category: P2025 category (e.g., "immigration", "environment")
            use_ollama: Use Ollama instead of OpenAI

        Returns:
            STORMReport
        """
        category_topics = {
            "immigration": (
                "How does Project 2025 propose to change immigration policy?"
            ),
            "environment": (
                "What environmental and EPA changes does Project 2025 recommend?"
            ),
            "education": (
                "How would Project 2025 restructure federal education policy?"
            ),
            "healthcare": "What healthcare and HHS reforms does Project 2025 propose?",
            "labor": (
                "How does Project 2025 plan to change labor regulations and unions?"
            ),
            "civil_rights": (
                "What civil rights and anti-discrimination changes does P2025 suggest?"
            ),
            "abortion": (
                "How does Project 2025 approach abortion and reproductive rights?"
            ),
            "guns": "What Second Amendment and firearm policies does P2025 support?",
            "federal_power": (
                "How does Project 2025 propose to reduce federal agency power?"
            ),
        }

        topic = category_topics.get(
            category,
            f"What are Project 2025's proposals for {category} policy?",
        )

        return self.generate_policy_report(
            topic=topic,
            use_custom_docs=True,
            use_ollama=use_ollama,
        )


def generate_storm_report(
    db_session: Session,
    topic: str,
    output_dir: str | None = None,
    use_ollama: bool = False,
) -> STORMReport:
    """Convenience function to generate a STORM report.

    Args:
        db_session: Database session
        topic: Topic to research
        output_dir: Output directory
        use_ollama: Use Ollama for LLM

    Returns:
        STORMReport
    """
    generator = STORMReportGenerator(db_session, output_dir=output_dir)
    return generator.generate_policy_report(
        topic=topic,
        use_custom_docs=True,
        use_ollama=use_ollama,
    )
